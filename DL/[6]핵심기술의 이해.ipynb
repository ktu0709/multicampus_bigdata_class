{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0dd2f24c",
   "metadata": {},
   "source": [
    "DL 핵심 기술 이해   \n",
    "\n",
    "1) 용어 변환  :  세트 -> 집합 , 기울기  -> 경사도  \n",
    "\n",
    "2) ML 부족한 이유  \n",
    "     - 규칙   복잡성 : 단순한 규칙 = 머신러닝 , 규칙이 복잡해 = 딥러닝 \n",
    "     - 데이터 복잡성 : 규칙 학습에 사용되는 데이터가 단순 = 머신러닝 , 규칙학습에 사용 되는 데이터가 복잡  = 딥러닝 \n",
    "     \n",
    "     \n",
    "3) 선형회귀  : 일종의 예측 기술 , 예측 모형 구축   = 머신, 딥 러닝에서 알고리즘 기반  \n",
    "   \n",
    "   - 데이터 점을 가장 잘 대표할 수 있는 예측선을 찾는 일 = 선형회귀 방정식 \n",
    "   \n",
    "   -  종속변수(X), 독립변수 (y) -> 데이터 점들을 가장 잘 대표하는 예측선의 기울기를 a라고 할 때  최소 제곱법으로 구한다. \n",
    "   \n",
    "   -  종속 변수가 여러개인 경우 평균 제곱근오차 (RMSE)가 가장 작은선을 대표하는 예측선  \n",
    "      => 먼저 예측선을 그리고 조금씩 수정하는 방법으로  RMSE로 결과를 확인 한다. \n",
    "      \n",
    "\n",
    "4) 경사하강법   \n",
    "    -  오차 최소화 기술을 의미\n",
    "    -  예측선의 기울기가 너무 커도, 작어도 오차가 커진다. \n",
    "    -  기울기를 x ,  오차를 y  = 2차 곡선 모양 (오목) \n",
    "     => 2차 곡선의 오차가 최소값(극소)일 때가 기울기가 가장 [적절한 기울기] 가 된다 \n",
    "     => 최소값(극소)을 찾으려면 미분을 해야 한다.  \n",
    "     => 미분한 값이 0이 될 때 최소에 해당한다.  \n",
    "     => 경사하강법이란 오차 곡선을 찾아 가면서 점점 미분의 계수를 줄여 나가는 것이다. \n",
    "\n",
    "\n",
    "5) 학습 속도 \n",
    "    - 한 번에 어느 정도의 폭으로 곡선을 따라 가는 가를 나타내는 용어이다.  \n",
    "    - 학습 속도가 너무 크거나 작으면 미분의 계수가 수렴되지 않고 발산한다.\n",
    "    - 적절한 학습 속도를 유지하면서 경사하강법을 쓰는 이유 : 다중 선형 회귀 문제를 해결할 수 있다. \n",
    "    \n",
    "6) 로지스틱 회귀 : 로지스틱 회귀를 통해  True / False  = 활성 / 비활성  을 판단할 수 있다. \n",
    "    - 시그모이드 함수  (s자형 함수 )를 이용해서 이항을 분류 한다. \n",
    "    - 시그모이드 함수에 나오는 공식중에  a, b의 값도 대칭 관계를 나타내는 로그 함수를 경사하강법으로 구할 수 있다. \n",
    "       \n",
    "7) 퍼셉트론 적용  -> 다중 퍼셉트론  -> 심층 심경망  \n",
    "   7-1  퍼셉트론 적용  :  신경 세포를 모방한 것 , 다중 선형회귀 방정식을  신경 세포를 모방해서 표현,논리곱,논리합만 연산  \n",
    "                         b= 절편   ->  bias 편향치  \n",
    "                         https://commons.wikimedia.org/wiki/File:Rosenblattperceptron.png <용어설명>\n",
    "                         \n",
    "                         \n",
    "   7-2  다중 퍼셉트론 :  연산 해결   _ 배타적 논리 합 문제를 해결\n",
    "          - XOR   : 마빈 민스킨 _ 네모난 바둑판 위에 힌돌과 검은 돌을 서로 교차해서 놓으면 단층 \n",
    "                    퍼셉트론으로는 구분할 구부선을 그을 수 없다. \n",
    "                    \n",
    "          - XOR= AND (NAND, OR) 로 은닉 계층을 하나 더 넣어서 2개 이상이 계층 퍼셉트론으로 문제를 해결 했다. \n",
    "          \n",
    "           \n",
    "   7-3  심층 심경망   : 다차원 특징 문제를 해결  \n",
    "          - 특징이 늘어 날 수록 폭과 깊이를 넓이는 방식  \n",
    "          - 깊이는 이론적으로 3개 계층 이상이 되며  은닉 계층이 2개 이상이라는 말이다. \n",
    "          - 너비는 이론적으로 3개 단위(유닛) 이상이 되며 , 입력 계층의 단위가 3개 이상이라는 말이다. \n",
    "          - 참고  = 은닉계층이 1개이고 입력계층 단위계수가 2개인것을 [퍼셉트론] 이라고 한다. \n",
    "          \n",
    "8) 오차 역전파   : 가중치 수정 문제가 발생한 것을 해결 하면서 학습한다. \n",
    "       -학습 : 시행 착오를 거쳐서 오차를 수정하는 일\n",
    "       - 신경망에서는 실측치(실제 값)과 예측치(예측값)을 비교하고 오차 역전파를 이용해서 오차를 수정하는 것을 한다. \n",
    "       - 신경망   : 순차, 역전파 \n",
    "        <<오차 역전파 알고리즘>>\n",
    "        > 초기 가중치를 임의로 부여 -> 가중치(w) 따라 결과를 계산한다.  -> 결과값의 오차 손실함수를 통해 비용함수 계산 \n",
    "         -> 경사하강법을 적용한다. (가중치를 오차가 작아지는 방향으로 갱신) \n",
    "         \n",
    "       <<경사도 >>\n",
    "        어떤 접면(hyperplane) 에서의 모든 벡터 방향으로의 도함수(미분 계수)의 크기를 말한다. \n",
    "        <<경사도 손실>>\n",
    "         미분값이 작아지다가 0에 가까운 값이 되는 것을 말한다. \n",
    "        <<판별>>\n",
    "         -오차를 역전파하면서 지속적으로 오차를 편미문 하므로 출력 계층에서 입력 계층 쪽으로 갈수록 경사도가 줄어든다. \n",
    "         - 활성 함수로 사용되는 시그모이드 함수 때문이다. \n",
    "       \n",
    "9) 변형함수= 활성화 함수     : 쌍곡 탄젠트 , 렐루, 소프트플러스 함수 순서로 개발 \n",
    "            <<렐루>> \n",
    "             계층을 더 깊게 쌓아서 연산 , 미분을 하면 활성함수값이 0보다 클때는 경사도 1, 0 보다 작을 때는 경사도가 0 인\n",
    "             미분함수 곡선이 그려진다. \n",
    "       \n",
    " 10) 경사 하강법 개선  : 계산량을 줄이는 목적(기존에는 경사도 갱신시마다 데이터를 미분했어야 했었다. )\n",
    " \n",
    "      확률적 경사 하강법(SGD)    :  일부 데이터만 임의 추출 해서 미분 \n",
    "      \n",
    "      -> 모멘텀(momentum)    : 관성을 적용해서 흔들림을 방지  \n",
    "      -> 네스트로프 모멘텀(nesterov)  : 불필요한 움직임을 삭제한  모멘텀 개선 \n",
    "      -> 에이다 그라드(Adagrad)  :  갱신 횟수가 늘면 학습 속도를 줄이는 목적  \n",
    "      -> RMSProp   :  Adagrad 를 민감하게 반응 하지 않도록 개선  \n",
    "      \n",
    "      \n",
    "      -> 아담(Adam : momentum+RMSProp  )     \n",
    "       \n",
    "       \n",
    " 11) 딥  러닝의 종류\n",
    "                   --> DNN ( Deep  Neural Network)  : 신경망(NN) 이라고 하는 패턴 인식을 하도록 설계된 \n",
    "                           것으로  사람이나 동물의 뇌신경 회로를 모델로 한 알고리즘을  다층 구조화 한 것 \n",
    "   \n",
    "                   -->CNN( Convlutional   Neural Network) _ 분류  : 데이터의 추상화 및 위치등을 제공하는 순 전파형\n",
    "                        신경망을 이용한 알고리즘, DNN을 2차원 데이터로 대응시킨것으로 화상에 높은 \n",
    "                          패턴 인식 능력을 표현 한다.    _  이미지 인식 \n",
    "\n",
    "                  --> RNN ( Recurrent Neural Network_ 재귀형 신경망 )  : 음성, 동영상   데이터와 같은 가변길이의\n",
    "                     데이터를  취급할 수 있도록 (중간층에 재귀적인 구조를 갖는다)  양방향으로 신호가 전파하는 \n",
    "                     신경망 알고리즘. \n",
    "                      DNN을 옆으로 연결해서 시간 변화를 연속적인 데이터에 대응시킨것으로 시간이 오려걸리는 데이터는\n",
    "                     적합하지 않다.    _  자연어 처리 , Google 번역 ,  음성인식 , 동영상인식        \n",
    "       \n",
    "       \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
