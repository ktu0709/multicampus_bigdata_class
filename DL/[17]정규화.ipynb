{
 "cells": [
  {
   "cell_type": "raw",
   "id": "416b8685",
   "metadata": {},
   "source": [
    "<< tf.keras.layers.BatchNormalization  확인  >>\n",
    "\n",
    "1 )미니 배치 마다 평균이  0, 분산이 1이 되도록 데이터의 정규화를 수행한다. \n",
    "\n",
    "2) 훈련 (학습)시는 미니배치의  평균, 분산을 사용해서 정규화 하지만 ,\n",
    "   추론시에는 일반적으로 이동 평균을 정규화 해서 사용한다.  \n",
    "   \n",
    "   ===> 학습중에 배치별로 각 특성의 평균과 분산을 계산하고, 출력을 정규화 시킨다\n",
    "   ===> 학습이 끝난 후에는 전체 데이터 셋의 이동평균과 이동분산을 사용하여 [평가난 예측]을 할때 입력을 정규화한다\n",
    "\n",
    "tf.keras.layers.BatchNormalization(\n",
    "    axis=-1, #특성 축, -1이면 마지막 차원, (batch,height,width,channels)\n",
    "    momentum=0.99, #이동평균 , 이동 분산의 업데이터에 사용하는 모멘텀값 -> 학습중에 평균과 분산의 가중치의 유지 비율 0~1\n",
    "    epsilon=0.001, #분모가 0이 되는 것을 방지,분산에 추가되는 상수\n",
    "    center=True, #베타항을 정규화된 텐서에 더해서 출력의 평균을 이동시킨다\n",
    "    scale=True, #감마 스케일 항을 정규화 텐서에 곱한다\n",
    "    beta_initializer='zeros', #베타 가중치 초기값 설정\n",
    "    gamma_initializer='ones', # 감마 가중치 초기값 설정\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones',\n",
    "    beta_regularizer=None, #정규화 적용하지 않겠다\n",
    "    gamma_regularizer=None, #감마 가중치에 적용할 정규화 함수\n",
    "    beta_constraint=None, #제약조건\n",
    "    gamma_constraint=None, #제약조건\n",
    "    **kwargs #부모생성자에 전달될 추가 인수 지정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f30c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taeeon.kim\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f6dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03602bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\taeeon.kim\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bn01 (BatchNormalization)   (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4 (16.00 Byte)\n",
      "Trainable params: 2 (8.00 Byte)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "_________________________________________________________________\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Q1) 정규 배치화를 사용한 모델을 만들어 보자.  \n",
    "model = tf.keras.Sequential([\n",
    "   \n",
    "    tf.keras.layers.BatchNormalization( name ='bn01' , input_shape=(1,))\n",
    "])\n",
    "model.summary() \n",
    "print (model.layers[0].trainable)\n",
    "\n",
    "#[확인 ]   trainable = True (훈련모드)  미니 배치의 평균 및 분산으로 정규화 하겠다.  gamma,beta가 업데이트된다\n",
    "# moving_mean ,# moving_variance -> 평균 및 분산의 이동 평균이 업데이트 된다.\n",
    "\n",
    "# trainable = False (추론 = 예측,분석모드)   \n",
    "# moving_mean ,# moving_variance -> \n",
    "#평균 및 분산의 이동 평균이 업데이트 되지 않은 상태에서  moving_mean,  moving_varianc로 정규화 하겠다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa6c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn01/gamma:0                  [1.]\n",
      "bn01/beta:0                   [0.]\n",
      "bn01/moving_mean:0            [0.]\n",
      "bn01/moving_variance:0        [1.]\n"
     ]
    }
   ],
   "source": [
    "#Q2) 모델이 가진 속성을 이용해서 가중치, 바이어스 등의 값을  리턴 받아 보자.  \n",
    "for w in model.trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "    \n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb18bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "bn01/gamma:0                  [1.]\n",
      "bn01/beta:0                   [0.]\n",
      "bn01/moving_mean:0            [0.]\n",
      "bn01/moving_variance:0        [1.]\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = False \n",
    "\n",
    "print(model.layers[0].trainable_weights )\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752546cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn01/moving_mean:0            [0.]\n",
      "bn01/moving_variance:0        [1.]\n",
      "[[100.]] (1, 1)\n",
      "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n",
      "bn01/moving_mean:0            [1.]\n",
      "bn01/moving_variance:0        [0.99]\n",
      "bn01/moving_mean:0            [99.99573]\n",
      "bn01/moving_variance:0        [4.273953e-05]\n",
      "1/1 [==============================] - 0s 250ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.13232422]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3) 실제 동작을 확인 해보자.     moving_mean_initializer='zeros',   moving_variance_initializer='ones',\n",
    "# training=True 일때 확인  \n",
    "model = tf.keras.Sequential([   \n",
    "    tf.keras.layers.BatchNormalization( name ='bn01' , input_shape=(1,))\n",
    "])\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "\n",
    "#3-1, 100개의 요소가 1개만 있는 데이터를 사용해보자.  (1,1)차원의 2차원으로  확인 \n",
    "a = np.array([[100]]).astype('float32')\n",
    "print(a , a.shape)\n",
    "\n",
    "#3-2  학습 모델로 호출하게 되면 출력은 초기의 0, 업데이트 되는 것을 확인 할  수 있다.  \n",
    "print(model(a, training=True))\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "\n",
    "#3-3  데이터를 계속 호출해서 데이터의 평균과 분산이  0에서 가까워 지는 것을 확인 하자.  \n",
    "for i  in range(1000):\n",
    "    model(a, training=True)\n",
    "    \n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "    \n",
    "model.predict(a)    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "26c93232",
   "metadata": {},
   "source": [
    "학습과정에서 계산되는 배치의 평균과 분산을 사용하여 업데이트 한다\n",
    "gamma[스케일 파라미터], beta[이동 파라미터] , moving_mean , moving_variance\n",
    "\n",
    "momentum -> 이동평균과 분산의 가중치 \n",
    "\n",
    "moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)\n",
    "                0  *0.99+ 100 *(1-0.99)  = 1\n",
    "\n",
    "moving_var = moving_var * momentum + var(batch) * (1 - momentum)\n",
    "                 1   *  0.99 +0*1(1-0.99)  = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be198780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn01/moving_mean:0            [0.]\n",
      "bn01/moving_variance:0        [1.]\n",
      "[[100.]] (1, 1)\n",
      "tf.Tensor([[99.95004]], shape=(1, 1), dtype=float32)\n",
      "bn01/moving_mean:0            [0.]\n",
      "bn01/moving_variance:0        [1.]\n",
      "bn01/moving_mean:0            [0.]\n",
      "bn01/moving_variance:0        [1.]\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "99.95003746877732\n"
     ]
    }
   ],
   "source": [
    "#Q4) 실제 동작을 확인 해보자.     moving_mean_initializer='zeros',   moving_variance_initializer='ones',\n",
    "# training=False 일때 확인  \n",
    "\n",
    "model = tf.keras.Sequential([   \n",
    "    tf.keras.layers.BatchNormalization( name ='bn01' , input_shape=(1,))\n",
    "])\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "\n",
    "#3-1, 100개의 요소가 1개만 있는 데이터를 사용해보자.  (1,1)차원의 2차원으로  확인 \n",
    "a = np.array([[100]]).astype('float32')\n",
    "print(a , a.shape)\n",
    "\n",
    "#3-2  학습 모델로 호출하게 되면 출력은 초기의 0, 업데이트 되는 것을 확인 할  수 있다.  \n",
    "print(model(a, training=False))\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "\n",
    "#3-3  데이터를 계속 호출해서 데이터의 평균과 분산이  0에서 가까워 지는 것을 확인 하자.  \n",
    "for i  in range(1000):\n",
    "    model(a, training=False)\n",
    "    \n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "    \n",
    "model.predict(a)\n",
    "print((100-0) / np.sqrt(1+0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc39ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0025\n",
      "bn03/gamma:0                  [1.]\n",
      "bn03/beta:0                   [0.]\n",
      "bn03/moving_mean:0            [0.]\n",
      "bn03/moving_variance:0        [1.]\n"
     ]
    }
   ],
   "source": [
    "#Q5)  training=False 일 경우 훈련 실행하는 fit() 은 추론 모드로 동작된다.  \n",
    "# fit()을 실행할 때   bn01/moving_mean, bn01/moving_variances는 갱신되지 않는다.\n",
    "\n",
    "model = tf.keras.Sequential([   \n",
    "    tf.keras.layers.BatchNormalization( name ='bn03' , input_shape=(1,))\n",
    "])\n",
    "\n",
    "model.layers[0].trainable =False #학습중에 가중치 업데이트 하지 않겠다 -> 가중치 고정\n",
    "\n",
    "model.compile(optimizer ='adam' , loss = 'mean_squared_error')\n",
    "model.fit(a,a)\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7442ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x000001F33368E340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 10000.0000\n",
      "bn04/moving_mean:0            [1.]\n",
      "bn04/moving_variance:0        [0.99]\n",
      "bn04/gamma:0                  [1.]\n",
      "bn04/beta:0                   [0.00099999]\n"
     ]
    }
   ],
   "source": [
    "#Q6)trainable =True\n",
    "model = tf.keras.Sequential([   \n",
    "    tf.keras.layers.BatchNormalization( name ='bn04' , input_shape=(1,))\n",
    "])\n",
    "\n",
    "model.layers[0].trainable =True\n",
    "\n",
    "model.compile(optimizer ='adam' , loss = 'mean_squared_error')\n",
    "model.fit(a,a)\n",
    "\n",
    "\n",
    "for w in model.non_trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "\n",
    "    \n",
    "for w in model.trainable_weights:\n",
    "    print('{:<30}{}'. format (w.name , w.numpy()))\n",
    "    \n",
    "    \n",
    "### 추론에서는 모든 모델의 모든 가중치가 고정되어야 한다\n",
    "### 전이 학습 시나리오에서는 BatchNormalization 레이어를 고정시켜 기존의 통계 (평균 및 분산)을 유지하면서 \n",
    "### 다른 레이어를 학습 시켜야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6a4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e97abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa74fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
